\chapter{Factorisation de Cholesky}

La méthode de Cholesky (ou Choleski) permet de résoudre des systèmes linéaires
dont la matrice est symétrique définie positive. On rappelle que
$A \in M_n(\R)$ est symétrique définie positive si et seulement si :
\begin{enumerate}
    \item $^tA = A$
    \item $^tXAX \geq 0, \forall x \in \R$
    \item $^tXAX = 0 \Rightarrow X = 0$
\end{enumerate}
Pour une matrice $A \in M_n(\R)$ symétrique, les propriétés 2) et 3) reviennent à supposer que toutes les valeurs propres de $A$ sont $> 0$.

On a également :
\begin{enumerate}
    \item $A$ définie positive $\Rightarrow$ $A$ inversible
    \item $A$ symétrique $\Rightarrow$ $A$ diagonalisable. $\exists P / A = P^{-1}DP$
    \item $A$ symétrique et définie positive $\Rightarrow$ toutes les valeurs propres de $A$ sont positives et réelles.
\end{enumerate}

Nous allons voir que les matrices pevuent se factoriser sous la forme $A = T\tpo{T}$ où
$T$ est triangulaire inférieure.

Pour résoudre $Ax = b$ ($x,b \in \R^n)$, càd $T\tpo{T}x=b$ on résout alors successivement :
\begin{equation*}
\left\lbrace
    \begin{array}{ccc}
        Ty  = & b & \text{ étape de ``descente''}\\
        \\
        \tpo{T}x  = & y & \text{ étape de ``remontée''}
    \end{array}
\right.
\end{equation*}

Cette méthode est bien sûr intéressante lorsqu'on doit résoudre plusieurs systèmes linéaires
avec des seconds membres $b$ différents, mais la même matrice $A$ (la factorisation est faite
une seule fois, et les étapes de descente et remontée pour chaque second membre).


\begin{ftheo}
    Soit $A \in M_n(\R)$ symétrique définie positive. Il existe (au moins) une matrice réelle triangulaire inférieure $T$ telle que :
    \[
        A = T \,^tT
    \]
    De plus, si on impose que les éléments diagonaux de T soient tous positifs, alors la factorisation $A = T \,^tT$ est unique.
\end{ftheo}


\begin{preuve}
    Les $n$ sous-matrices 
$\Delta _i =
\begin{pmatrix}
    a_{11} & \dots & a_{1i} \\
    \vdots & & \vdots \\
    a_{i1} & \dots & a_{ii}
\end{pmatrix}$
sont inversibles car elles sont symétriques définies positives. En effet, si 
$x =
\begin{pmatrix}
    x_1 \\
    \vdots \\
    x_i
\end{pmatrix}
$
et
$X =
\begin{pmatrix}
    x_1 \\
    \vdots \\
    x_i \\
    0 \\
    \vdots \\
    0
\end{pmatrix}
$
on a $\tpo{x} \Delta_i x = \tpo{X}AX$

Donc, d'après le théorème 2 du chapitre précédent, on a $A = LU$ avec $L$ triangulaire
inférieure de diagonale unité, et $U$ triangulaire supérieure inversible $(\rightarrow u_{ii} \ne 0 \forall i)$.
Notons $D = \diag{u_{ii}}$

On a : $A = \tpo{A} = \tpo{U} \tpo{L} = \underbrace{\tpo{U} D^{-1}}_\text{triang. inf de diagonale unité \hspace{0.2cm}}
\times
\underbrace{D \tpo{L}}_\text{triangulaire supérieure}$

Par unicité de la décomposition $LU$ il vient $D\tpo{L} = U$ et donc $A = L D \tpo{L}$. De plus, si $\tpo{L} V_i = e_i$

\[
    0 < \tpo{V_i} \, A \, V_i = \tpo{V_i} \, L \; D \; \tpo{L} \, V_i = \tpo{e_i} \, D \, e_i = u_{ii}
\]

Notons maintenant $\sqrt{D} = \diag{\sqrt{u_{ii}}}$. Alors $A = L \, \sqrt{D} \, \tpo{\sqrt{D}} \, \tpo{L}$,
c'est-à-dire $A = T \tpo{T}$ avec $T = L \sqrt{D}$.

L'unicité de la factorisation $A = T \tpo{T}$ vient de l'algorithme de calcul de $T$ décrit plus loin.
\end{preuve}


\subsection*{Calcul de T :}

T peut être calculé à partir d'une factorisation LU, mais on expose ici une méthode moins coûteuse en temps de calcul.

Puisque $A = T \tpo{T}$ avec $T$ triangulaire inférieure :

\[
    a_{ij} = \sum_{k = 1}^{\Min(i, j)} t_{ik} \cdot t_{jk}
\]

\begin{enumerate}[label=-]
    \item Calcul de la colonne 1 de $T$ :
        \[
            a_{11} = t_{11}^2 \implies \large{\fbox{$t_{11} = \sqrt{a_{11}}$}} \hspace{1cm} 
            \text{($a_{11} > 0$ car $A$ symétrique définie positive)}
        \]
        Pour $i \geq 2$ :
        \[
            a_{i1} = t_{i1} \cdot t_{11} \implies \Large{\fbox{$t_{i1} = \frac{a_{i1}}{t_{11}}$}}
        \]
    \item Supposons connues les colonnes $1$ à $p$ de $T$.

Calcul la colonne $p+1$ :
        \begin{align*}
            & a_{p+1,p+1} = t_{p+1,p+1}^2 + \sum_{1 \leq k \leq p}(t_{p+1,k})^2 \to
            \text{calculés précédemment (colonnes 1 à $p$)} \\
            \implies & t_{p+1,p+1}^2 = a_{p+1,p+1} - \sum_{1 \leq k \leq p}(t_{p+1,k})^2 > 0 \hspace{0.5cm} \text{\underline{puisque $T$ existe}} \\
            \implies & t_{p+1,p+1} = \left(a_{p+1,p+1} - \sum_{1 \leq k \leq p}(t_{p+1,k})^2\right)^{\frac{1}{2}}
        \end{align*}

        Pour $i \geq p+2$ :
        \begin{equation*}
            \begin{split}
                & a_{i,p+1} = t_{i,p+1} \cdot t_{p+1,p+1} + \sum_{1 \leq k \leq p}(t_{ik} \cdot t_{p+1,k})
            \hspace{1cm} \to \text{(calculés précédemment)} \\
                \implies & t_{i,p+1} = \frac{1}{t_{p+1,p+1}} \left ( a_{i,p+1} - \sum_{1 \leq k \leq p} t_{ik} \cdot t_{p+1,k} \right )
            \end{split}
        \end{equation*}

\end{enumerate}

\subsection*{Coût du calcul de T :}
On assimile l'extraction de racine carrée à une opération arithmétique élémentaire (ce qui est
une approximation, car cette opération est plus compliquée que les opérations élémentaires
$\times, +, -$ \dots).

Calcul de la 1\up{ère} colonne de $T$ : $n$ opérations.

Calcul de la $(p+1)$\up{ème} de $T$ :

\begin{enumerate}[label=-]
    \item Calcul de $t_{p+1,p+1}$ :
        \[
            \left.
            \parbox{4cm}{
                \begin{enumerate}[label=*]
                    \item $p$ multiplications
                    \item $p$ soustractions
                    \item $1$ racine
                \end{enumerate}
            }\right\rbrace \text{$2p+1$ opérations}
        \]

    \item Calcul de $t_{i,p+1} (p+2 \leq i \leq n)$ :
        \[
            \left.
            \parbox{4cm}{
                \begin{enumerate}[label=*]
                    \item $p$ multiplications
                    \item $p$ soustractions
                    \item $1$ divison
                \end{enumerate}
            }\right\rbrace \text{$2p+1$ opérations}
        \]
\end{enumerate}

$\to (n-p) \times (2p+1)$ opérations pour le calcul de la colonne $p+1$.

\begin{align*}
    \text{Coût total} & = \sum_{p=0}^{n-1} (n-p)(2p+1) \\
    & = (2n-1) \Bigg( \sum_{p=1}^{n-1}p \Bigg) + n^2 - 2 \sum_{p=1}^{n-1} p^2 \\
    & \sim_{n \to + \infty} n^3 - \frac{2}{3} n^3
\end{align*} 

Donc nombre d'opérations élémentaires $\sim_{n \to +\infty} \frac{1}{3}n^3$.
($\sim \frac{1}{6}n^3$ multiplications et $\frac{1}{6}n^3$ soustractions)


\subsection*{Coût de la résolution de $Ax = b$}

Le coût des étapes des descente et de remontée est $\mathcal{O}(n^2)$.

L'étape la plus coûteuse est donc la factorisation $A = T \tpo{T}$, et le coût total est $\sim \frac{1}{3}n^3$. C'est donc mieux que Gauss (coût $\sim \frac{2}{3}n^3)$.





