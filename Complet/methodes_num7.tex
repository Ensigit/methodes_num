\chapter{Optimisation sans contrainte}

Étant donné $f : \R^n \longrightarrow \R$ suffisamment régulière (typiquement
$\mathcal{C}^2$), nous allons étudier d'un point de vue analytique et
numérique l'existence d'extrema de $f$ (un extremum = un minimum puis un
maximum). On parle d'optimisation \textbf{sans contrainte} (un problème 
d'optimisation avec contrainte étant posé sur un sous-ensemble de $\R^n$).
Il s'agira pour nous de trouver un \textbf{minimum} d'une fonction f, sous perte
de généralité car le maximum d'une fonction $\tilde{f}$ revient à chercher
un minimum de $f = - \tilde{f}$.

\subsubsection*{Exemples :}

\begin{enumerate}[label=-]
    \item Résolution d'un système linéaire $Ax=b$, avec $A$ symétrique définie
        positive. Exemple d'application : résolution de l'équation de Poisson
        par différences finies. Nous verrons que la solution du système
        minimise $f(x) = \frac{1}{2} \tpo{x}Ax - \tpo{b}x$. Cette propriété
        permet d'introduire de nouvelles méthodes de résolution du système.

    \item Exemple de minimisation d'une fonction non quadratique :
        \[
            \begin{array}{cc}
                f(x) = \dfrac{1}{2} \underset{1\leq i,j\leq N, i\ne j}{\sum} V(\norm{x_i - x_j}) 
                & 
                \parbox{5cm}{Cristal constitué de $N$ atomes, de positions $x_i \in \R^3$,\\ interagissent par paires via un potentiel $V$}
            \end{array}
        \]
        
        $f$ représente l'énergie potentielle totale du cristal. La forme d'équilibre du
        cristal à $T = 0$ Kelvin est donnée par un minimum de l'énergie potentielle $f$.

        Sous certaines hypothèses sur $v$ et en deux dimensions d'espace, il a été démontré
        très récemment (F. Theil, 2005) que ce minimum est atteint pour un arrangement
        périodique des atomes.
\end{enumerate}

:x:q

\section{Quelques méthodes numériques pour l'optimisation sous contraintes :}

Nous abordons maintenant le calcul numérique d'un minimum de $\deffonc{f}{\R^n}{\R}$de classe $\Co^1$. On suppose que $\underset{\norm{x} \longrightarrow +\infty}{\lim} f(x) = +\infty$, de sorte que ce minimum existe.

Nous allons d'abord voir des \textbf{méthodes de gradient}, qui sont des
algorithmes itératifs utilisant uniquement $f$ et $\nabla f$.

L'exemple le plus simple d'une telle méthode est l'algorithme du
\underline{\textbf{gradient à pas constant}} :

\[
    \left\lbrace
    \begin{array}{cc}
        x_{k+1} = x_k - \rho \nabla f(x_k) & \hspace{1cm} \rho > 0 \text{ fixé} \\ [5pt]
        x_0 \in \R^n \text{ donné}
    \end{array}
    \right.
\]


Cette méthode est motivée par la propriété que $-\nabla f$ est orienté dans
le sens des valeurs de $f$ décroissantes.

On dit alors que $- \nabla f(x_k)$ est une \textbf{direction de descente} en $x_k$.

La méthode du gradient à pas constant est assez peu utilisée en pratique
car elle conduit facilement à des instabilités numériques. Par exemple,
pour $f(x) = x^4$ (fonction strictement convexe) on obtient $x_{k+1} = x_k(1-4 \rho x_k^2)$. Si $x_0^2 \geq \frac{1}{\rho}$, on montre par récurrence que
$|x_{k+1}| \geq 3 |x_k|$ (car $1-4\rho x_k^2 \leq - 3$) et donc $|x_k| \longrightarrow_{k\to +\infty} + \infty$.

Pour éviter ce type de phénomène, on peut considérer la \textbf{méthode de la
plus grande pente} (ou steepest descent method) dans laquelle $\rho$ est adapté
à chaque itération de manière \textbf{optimale} :

\[
    \begin{array}{cc}
        \left\lbrace
        \begin{array}{c}
            x_0 \in \R^n \text{ donné} \\[5pt]
            x_{k+1} = x_k - \rho_k \nabla f (x_k)
        \end{array}
        \right.
        & \hspace{1cm}
        f\left(x_k - \rho_k \nabla f(x_k)\right) = \MinI{\rho \geq 0} f \left(x_k - \rho \nabla f(x_k) \right)
    \end{array}
\]

À chaque étape de l'itération, il faut donc résoudre un problème de minimisation
en une dimension; plus précisément minimser la fonction $\deffonc{\phi}{\R^+}{\R}$ :
\[
    \rho \mapsto f(x_k - \rho \nabla f(x_k) ) := \phi (\rho)
\]
un minimum étant atteint en $\rho = \rho_k$ (le minimum existe sans être
nécessairement unique) puisque $\underset{\norm{x} \to +\infty}{\lim} f(x) = +\infty$.

\subsubsection*{Calcul de $\rho_k$ :}
Il y a plusieurs possibilités.
\begin{enumerate}[label=•]
    \item Méthode de Newton ou méthode de la sécante pour résoudre
        $\phi'(\rho) = 0$. Noter que c'est une condition nécessaire mais
        en général non suffisante pour obtenir un minimum.

        Cependant, si $f$ est convexe alors $\phi$ est aussi convexe (c'est
        la restriction de $f$ à une droite passant par $x_k$).

    Dans ce cas $\phi'(\rho) = 0 \iff \phi(\rho) = \MinI{y \in ]0,+\infty[}{\phi(y)}$

    \item Posons $a = 0$.
        
        On suppose $\deffonc{\phi}{[a,b]}{\R}$ unimodale, c.à.d. 
        \[
            \exists \rho^
        \in ]a,b[ \text{ tel que } \phi' < 0 \text{ sur } ]a,\rho^*[ \text{ et } \phi' > 0 
        \text{ sur } ]\rho^*, b[. 
        \]
        
        On pose $\delta = \dfrac{b-a}{4}, x_i = a + i\delta$.

        Selon la position relative des $f(x_i)$ ($i = 1,2,3$) on peut choisir
        $a' < b'$ tels que $f$ est unimodale sur $[a',b'] \subset [a,b]$ et
        $b' - a' = \frac{1}{2} (b - a)$. On recommence l'opération sur
        $[a',b']$ jusqu'à atteindre la précision souhaitée.


    \item \textbf{Cas particulier d'une fonction quadratique :}
        \[
            f(x) = \frac{1}{2} \tpo{x} \; A \; x - \tpo{b} \; x
        \]
    $A \in M_n(\R)$ symétrique définie positive, $b \in \R^n$.

    Notons $r_k = \nabla f(x_k) = A x_k - b \neq 0$ (sinon le min
    est déjà atteint !)

    $\phi'(\rho_k) = 0 \iff r_{k+1} . r_k = 0 \iff \underbrace{A x_k - \rho_k \: A \: r_k }_{Ax_{k+1}} - b) . r_k = 0$

    On obtient donc explicitement : 
    \[
        \rho_k = \frac{\norm{r_k}^2_2}{\tpo{r_k} \: A \: r_k}
    \]
    avec $\tpo{r_k} \: A \: r_k \neq 0$ puisque $A$ est symétrique définie
    positive.
\end{enumerate}

\begin{remark}
    En pratique le calcul de $p_k$ n'a pas besoin d'être réalisé avec
    une très grande précision.
\end{remark}

\noindent
\begin{tabular}{||c}
\begin{minipage}[c]{15cm}
        On peut montrer que la méthode de la plus grande pente converge pour toute
        condition initiale $x_0$ si $x$ est strictement convexe. La convergence
        est linéaire et peut donc être assez lente.
    \end{minipage}
\end{tabular}

\vspace{0.3cm}

\noindent
\begin{tabular}{||c}
    \begin{minipage}[c]{15cm}
        Pour avoir ue convergence plus rapide, on peut utiliser la méthode de
        Newton pour résoudre $\nabla f(x) = 0$. En particulier, si f est
        convexe on obtient ainsi forcément un minimum de $f$. Il existe par
        ailleurs des variantes moins coûteuses que Newton et efficaces, comme
        la méthode de Broyden.
    \end{minipage}
\end{tabular}

Une autre méthode beaucoup utilisée est la \textbf{méthode du gradient conjugué}.

Soit $\deffonc{f}{\R^n}{\R}$ de classe $\Co^2$, avec $f(x) \xrightarrow{\norm{x} \to +\infty} +\infty$
et $H_f(x)$ symétrique définie positive $\forall x \in \R^n$.

$f$ possède alors un minimum global strict $\overline{x} \in \R^n$. La méthode
du gradient conjugué utilise une direction de descente plus efficace que
$\nabla f(x_k)$, qui fait également appel à $\nabla f(x_{k-1})$. Nous allons
étudier cette méthode lorsque $f$ est une fonction quadratique mais elle
s'applique dans un cadre plus général.

\section{Méthode du gradient conjugé pour une fonction quadratique}

On considère $f(x) = \frac{1}{2} \tpo{x} \: A \: x - \tpo{b} \: x$ avec
$A \in M_n(\R)$ symétrique définie positive et $b \in \R^n$. Nous avons
vu que $\deffonc{f}{\R^n}{\R}$ admet un minimum global strict en $x = \overline{x}$ avec $A \overline{x} = b$.

La méthode du gradient conjugué définit une suite $(x_k)_{k\geq 0}$ qui
converge vers $\overline{x}$. Nous allons voir que la convergence se fait
en \textbf{un nombre fini d'itérations $\leq n$};  de ce point de vue,
la méthode du gradient conjugué est donc à classer parmi les méthodes
directes. Cependant, à cause des erreurs d'arrondis, cette propriété
n'est pas vérifiée en pratique (plus particulièrement pour de grands
systèmes) et la méthode est plutôt considérée comme itérative. On
contrôlera donc cet algorithme par un nombre maximal d'itérations et
par un test d'arrêt.

\subsection{Description de la méthode :}

\noindent
\begin{tabular}{||c}
    \begin{minipage}[c]{15cm}
        On notera par la suite $r_k = \nabla f(x_k) = A x_k - b$. Si
        $r_k = 0$ alors l'algorithme s'arrête ($x_k = ?$ illisible).
    \end{minipage}
\end{tabular}

\begin{enumerate}[label=\textit{\roman*})]
    \item \textbf{Initialisation :}
        On fixe $x_0 \in \R^n$.
        \begin{enumerate}[label=-]
            \item Si $x_0 = 0$ alors l'algorithme s'arrête car $x_0 = \overline{x}$.
            \item Si $x_0 \neq 0$, on calcule $x_1$ par la méthode de plus grande pente.

                On pose $\omega_0 = \nabla f(x_0)$. 
                
                $-\omega_0 =$ direction de pente pour calculer $x_1$.
                
                $x_1 = x_0 - \rho_0 \omega_0$, \hspace{0.5cm}
                $f(x_0 - \rho_0 \omega_0) = \MinI{\rho \geq 0} f(x_0 - \rho \omega_0)$

                \begin{remark}
                    Minimum explicite car minimse un polynôme de degré 2 en $\rho$.
                \end{remark}
        \end{enumerate}

    \item \textbf{Itération :}
        On suppose connus $x_k$ et $\omega_{k-1}$ ($-\omega_{k-1}$ est la direction de la pente
        utilisée pour calculer $x_k$).

        \begin{enumerate}[label=-]
            \item Si $r_k = 0$ alors l'algorithme s'arrête car $x_k = \overline{x}$.

            \item Si $r_k \neq 0$ : on pose
                \begin{align}
                    \omega_k & = r_k + \theta_k \omega_{k-1} \notag\\ 
                    \theta_k & = \frac{\tpo{r_k} (r_k - r_{k-1}}{\norm{r_{k-1}}_2^2}
                    \label{eq-optim:iteration}
                \end{align}
                ($-\omega_k =$ direction de la descente pour calculer $x_{k+1}$)
        \end{enumerate}

        $x_{k+1} = x_k - \rho_k \omega_k, \hspace{0.5cm} f(x_k - \rho_k \omega_k) = \MinI{\rho \geq 0} f(x_k - \rho \omega_k)$

        Dans le cas présent où $f$ est quadratique, la valeur de $\rho_k$ est
        connue explicitement (voir le lemme qui suit).
\end{enumerate}

Nous allons montrer les résultats suivants : (en patriculier, $r_k \neq 0$ implique
$\omega_k \neq 0$ puisque $r_k \perp \omega_{k-1}$


\begin{lemme}
    \begin{enumerate}[label=\textit{\roman*)}]
        \item $f(x_{k+1}) = \MinI{\theta \in \R} \MinI{\rho \geq 0} f \Big[ x_k - \rho(r_k + \theta \omega_{k-1} \Big]$

        \item $\tpo{r_k} w_{k-1} = 0, 
            \hspace{0.2cm} 
            \rho_k = \dfrac{\norm{r_k}^2_2}{\tpo{\omega_k} A \omega_k}$

        \item $\tpo{\omega_k} \: A \: \omega_{k-1} = 0 \hspace{0.5cm} (w_k \text{ et }
            \omega_{k-1} \text{ sont dits ``$A$-conjugés''})$
    \end{enumerate} 

    \label{lemme1}
\end{lemme}

\begin{lemme}
    $\tpo r_k \: r_{k-1} = 0$ et (\ref{eq-optim:iteration}) se transforme en :
    \begin{align}
        \theta_k = \frac{\norm{r_k}_2^2}{\norm{r_{k-1}}_2^2}
        \label{eq-optim:eqlemme}
    \end{align}
    \label{lemme2}
\end{lemme}

\begin{remark}
    Les formules (\ref{eq-optim:iteration}) et (\ref{eq-optim:eqlemme}) sont équivalentes pour
    une fonction $f$ quadratique. Pour $f$ plus générale, (\ref{eq-optim:iteration}) correspond
    à la méthode de Polak-Ribière et (\ref{eq-optim:eqlemme}) à celle de Fletcher-Reeves.
    La méthode du gradient conjugué dans le cas quadratique est dûe à Hestenes et
    Steifel (1952).
\end{remark}

\subsection{Preuve du lemme \ref{lemme1}}
Nous allons montrer successivement \textit{ii}), \textit{i}) et \textit{iii}).

Tout d'abord, puisque $f(x_{k-1} - \rho_{k-1} \omega_{k-1}) = \MinI{\rho \geq 0} f(x_{k-1} - \rho \omega_{k-1})$

On a $\nabla f(x_{k-1} - \rho_{k-1} \omega_{k-1}) - \omega_{k-1} = 0$, soit
$r_k \: \omega_{k-1} = 0 \implies \text{on a montré \textit{ii}) 1\up{ère} égalité.}$

Pour $\omega = r_k + \theta \omega_{k-1}$ on a (polnyôme du second degré en $\rho$.

\begin{align*}
    f(x_k - \rho \omega & = f(x_k) - \rho \nabla f(x_k) \: \omega + \frac{1}{2} \rho^2 \tpo \omega \: H_f(x_k) \: \omega \\
    & = f(x_k) - \rho \: r_k \: \omega + \frac{1}{2} \rho^2 \tpo \omega \: A \: \omega
\end{align*}

Puisque $r_k \: \omega_{k-1} = 0$, $r_k \: \omega$ est indépendant de $\theta$ et on obtient :
\begin{align}
    f(x_k - \rho \omega) = f(x_k) - \rho \norm{r_k}^2 + \frac{1}{2} \rho^2 \tpo \omega \: A \: \omega
    \label{eq-optim:4}
\end{align}

Le minimum de ce polynôme de degré 2 est atteint en :
\[
    \rho_{\theta} = \frac{\norm{r_k}^2_2}{\tpo \omega \: A \: \omega} \hspace{1cm}
    \text{2\up{ème} égalité de \textit{ii})}
\]

et vaut 
\[
    f(x_k - \rho_\theta \omega) = f(x_k) - \frac{1}{2} \frac{\norm{r_k}_2^4}{\tpo \omega \: A \: \omega}
\]

Pour minimiser $f(x_k - \rho_\theta \omega)$ suivant $\theta$ il faut minimiser 
$\tpo \omega \: A \: \omega$, c'est à dire $|\omega|$.
Il faut choisir pour cela $\omega = \omega_k$ tel que $\tpo \omega_k \: A \: \omega_{k-1} = 0$, ce qu'on notera $\omega_k \perp \omega_{k-1}$ :
\[
    < r_k + \theta \omega_{k-1}, r_k + \theta \omega_{k-1} = |r_k|^2 + 2 \theta <r_k, \omega_{k-1} >
    + \theta^2 |\omega_{k-1}|^2
\]

Minimum pour :
\begin{align}
    \theta = \theta_k = - \frac{<r_k, \omega_{k-1}>}{|\omega_{k-1}|^2}
    \label{l1:eq5}
\end{align}

Donc :
\begin{align}
    \omega_k = r_k - \omega_{k-1} \frac{<r_k,\omega_{k-1}>}{|w_{k-1}|^2}
    \label{l1:eq6}
\end{align}

D'où $\omega_k \perp \omega_{k-1}$. Afin de montrer le lemme \ref{lemme1}, il reste à
montrer que (\ref{l1:eq5}) correspond bien à (\ref{eq-optim:iteration}).
D'une part :
\begin{align}
    r_k - r_{k-1} = A \: (x_k - x_{k-1}) & = - \rho_{k-1} \: A \: \omega_{k-1} \hspace{0.5cm}\text{ donc :} \notag \\
    \tpo r_k (r_k - r_{k-1}) & = - \rho_{k-1} <r_k,\omega_{k-1}>
    \label{l1:eq7}
\end{align}

D'autre part :
\begin{align*}
    |\omega_{k-1}|^2 & = (A \omega_{k-1}, \omega_{k-1}) = -\frac{1}{\rho_{k-1}}(A (x_k - x_{k-1}), \omega_{k-1}) \\
    & = -\frac{1}{\rho_{k-1}}(r_k - r_{k-1} , \omega_{k-1}) \\
    & = \frac{1}{\rho_{k-1}}(r_{k-1}, \omega_{k-1}) \hspace{2cm} \text{(car $(r_k,\omega_{k-1}) = 0$)}\\
    & = \frac{1}{\rho_{k-1}}(r_{k-1},r_{k-1}- \theta_{k-1}\omega_{k-2}) \\
    & = \frac{1}{\rho_{k-1}}\norm{r_k}^2 \hspace{2cm} \text{(car $(r_{k-1}, \omega_{k-2}) = 0$)}
\end{align*}

Donc :
\begin{align}
    \norm{r_k}^2 = \rho_{k-1} |\omega_{k-1}|^2
    \label{l1:eq8}
\end{align}

Avec (\ref{l1:eq5}), (\ref{l1:eq7}) et (\ref{l1:eq8}) on obtient donc :
\[
    \frac{\tpo r_k (r_k - r_{k-1}}{\norm{r_{k-1}^2}}  - \frac{<r_k,\omega_{k-1}>}{|\omega_{k-1}|^2} = \theta_k
\]

On obtient donc la formule (\ref{eq-optim:eqlemme}) plus simple pour le calcul de $\theta_k$.

\subsection{Convergence de la méthode du gradient conjugué et preuve du \ref{lemme2}}

Supposons $r_k \ne 0$ pour $k = 0,\dots,n-1$ (si $r_k$ s'annule l'algorithme converge). Cela implique
$\rho_k \ne 0$ pour $k=0,\dots,n-1$.

\begin{lemme}
    Pour tout $k=1,\dots,n$ on a :
    \[
        \begin{array}{cc}
            (P_k) &
            \left\lbrace
            \begin{array}{cc}
                r_k \: \omega_q = 0 & \text{pour $q = 0,\dots,k-1$} \\
                \tpo \omega_k \: A \omega_q = 0 & \text{pour $q = 0,\dots,k-1$} \\
                r_k \: r_q = 0 & \text{pour $q = 0,\dots,k-1$} \\
            \end{array}
            \right.
        \end{array}
    \]
    \label{lemme3}
\end{lemme}

\begin{preuve}
    Par récurrence. On considère les produits scalaires 
    \[
        \left\lbrace
        \begin{array}{ccc}
            (x,y) & = & \tpo xy = x.y \\
            & \text{et} & \\
            <x,y> \: & = & \tpo x \: A \: y
        \end{array}
        \right.
    \]

    \begin{enumerate}[label=•]
        \item $P_1$ est vraie : $r_1 \: r_0 = r_1 \: \omega_0 = 0$ (condition d'optimalité de $\rho_0$)

            \[
                < \omega_1 , \omega_0 > = 0 \hspace{1cm} \text{d'après le lemme 1}
            \]

        \item Supposons $P_k$ vraie et montrons $P_{k+1} (k \leq n-1)$. On a :

            \[
                r_{k+1}.\omega_k = 0 \hspace{1cm} \text{(condition d'optimalité de $\rho_k$)}
            \]

            \begin{align*}
                r_{k+1}.\omega_q & = (Ax_{k+1} - b, \omega_q) = \left(A(x_{k+1} - x_k) + Ax_k - b, \omega_q \right) \\
                & = - \rho_k <\omega_k,\omega_q> + r_k . \omega_q \\
                & = 0 \hspace{0.5cm} \text{pour $q=0,..,k-1$ (hyp de récurrence $P_k$)}
            \end{align*}

            Donc $r_{k+1}.\omega_q = 0$ pour $q=0,..,k$.

            Par ailleurs, $r_{k+1}.r_q = r_{k+1}.(\omega_q - \theta_q \omega_{q-1})$ \hspace{0.3cm} (avec $\theta_0 := 0$ car $r_0 = \omega_0$)

            \vspace{0.4cm}
            Donc $r_{k+1}.r_q = 0$ pour $q=0,..,k$

            \vspace{0.4cm}
            Ensuite $<\omega_{k+1},\omega_k> = 0$ d'après le lemme \ref{lemme1}, et pour
            $q=0,\dots,k-1$ :
            \[
                <\omega_{k+1},\omega_q> = <r_{k+1},\omega_q> + \theta_{k+1} <\omega_k,\omega_q> = <r_{k+1},\omega_q>
            \]
            (par l'hypothèse de récurrence $P_k$)

            \vspace{0.4cm}
            $\rho_{q+1} - r_q = A(x_{q+1}-x_q) = - \rho_q \; A \; \omega_q$ amène alors :
            \begin{align*}
                <\omega_{k+1},\omega_q> & = <r_{k+1}, \omega_q> = \frac{1}{\rho_q}(r_{k+1},-r_{q+1}+r_q) \\
                & = 0
            \end{align*}
            car $0 \leq q \leq k-1$ et $(r_{k+1},r_p) = 0$ pour $p = 0,\dots,k$

            Cela prouve $P_k$ par récurrence.
    \end{enumerate}
\end{preuve}

En conclusion, la famille $(\omega_0, \dots, \omega_{n-1})$ est libre car les $\omega_i$ sont
deux à deux orthogonaux pour le produit scalaire $<x,y> = \tpo x \; A \; y$. C'est donc une
base de $\R^n$. Puisque $r_n$ est orthogonal à $\omega_0, \dots, \omega_{n-1}$, on a donc
$r_n=0$. Nous avons donc montré que $A \, x_n = b$, i.e. $x_n=??$

\begin{ftheo}
    Soit $A \in M_n(\R)$ symétrique définie positive, $b \in \R^n$ et ${f(x) = \frac{1}{2} \tpo x \: A \: x - \tpo b \: x}$.
    Alors l'algorithme du gradient conjugué définit une suite $(x_k)_{k=0,\dots,p}$ avec
    $p \leq n$ et $A \: x_p = b$. On a $f(x_p) = \MinI{x \in \R^n} f(x)$.
\end{ftheo}

\begin{remark}
    Le cas $p<n$ est exceptionnel.
\end{remark}

Enfin, nous avons montré dans le lemme \ref{lemme3} que $r_k.r_{k-1}=0$, ce qui prouve le lemme
\ref{lemme2}.

